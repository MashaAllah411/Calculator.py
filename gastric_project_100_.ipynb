{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MashaAllah411/Calculator.py/blob/main/gastric_project_100_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "up5_sIAEpSmM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "MRERD5vRFwrf",
        "outputId": "d5ebc770-46b7-470c-eb5b-5370b0ddd631"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2621109975.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# which provides functionality for interacting with Google Drive within the Colab environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "# which provides functionality for interacting with Google Drive within the Colab environment.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA1NpYbvBOx2"
      },
      "source": [
        "Importing all the requires modules and packages\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "KXCktH_IpUcF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srZEZAq3wviF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import json\n",
        "import imageio.v2\n",
        "import random\n",
        "import imageio\n",
        "import warnings\n",
        "import PIL.Image\n",
        "import imutils\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from itertools import chain\n",
        "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
        "from skimage.transform import resize\n",
        "from skimage.morphology import label\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# Different layers and functionalities from the Keras library for building neural network architectures.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.layers import MaxPooling2D,Conv2D,Concatenate,Conv2DTranspose,AveragePooling2D,ZeroPadding2D,Activation,BatchNormalization,Dense,Flatten,Input,add,Dropout,Lambda\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score,precision_score,recall_score,f1_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO88_7sYKiV5"
      },
      "outputs": [],
      "source": [
        "# Create a New folder for storing Normal,Stage 1 and Stage2 Mask Image\n",
        "classes=['normal','stage1','stage2']\n",
        "path1='/content/gdrive/MyDrive/Gastric_Cancer_Project'\n",
        "path = os.path.join(path1, 'masks')\n",
        "os.mkdir(path)\n",
        "for i in classes:\n",
        "    datapath=path+'/'\n",
        "    path2=os.path.join(datapath, i)\n",
        "    os.mkdir(path2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9T0rfOEIcKrn"
      },
      "source": [
        "# Generate mask image for normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s-wnnztF4p3"
      },
      "outputs": [],
      "source": [
        "# The code creates mask images for the 'normal' class based on the images provided in the\n",
        "# '/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/normal/' directory.\n",
        "# It initializes empty mask images (img4) with the same dimensions as the original images and saves\n",
        "# them in the '/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/normal/' directory.\n",
        "# These masks are binary images with all pixel values set to zero.\n",
        "\n",
        "\n",
        "# creating normal mask images from annotation\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/normal\"):\n",
        "    for filename in files:\n",
        "        # print(filename)\n",
        "        img = np.asarray(PIL.Image.open('/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/normal/'+filename))\n",
        "        img4 = np.zeros((img.shape[0],img.shape[1]))\n",
        "        cv2.imwrite('/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/normal/%s' % filename,img4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihCND8asNzwz"
      },
      "source": [
        "# Generate mask image for Stage-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh_MnyKkrYbo"
      },
      "outputs": [],
      "source": [
        "# The code generates stage-1 mask images from annotations provided in a JSON file (stage1cancer.json).\n",
        "# It iterates through image files, extracts region coordinates, plots contours on the original image,\n",
        "# and creates corresponding binary masks. These masks are saved in the '/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/stage1/' directory in PNG format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# creating stage-1 mask images from annotation\n",
        "with open(\"/content/gdrive/MyDrive/Gastric_Cancer_Project/stage1.json\", \"r\") as read_file:\n",
        "    data = json.load(read_file)\n",
        "\n",
        "all_file_names=list(data.keys())\n",
        "# print(len(all_file_names))\n",
        "\n",
        "files_in_directory = []\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/stage1\"):\n",
        "    for filename in files:\n",
        "        files_in_directory.append(filename)\n",
        "\n",
        "a=0\n",
        "for j in range(len(all_file_names)):\n",
        "    # print(all_file_names[j])\n",
        "    image_name=data[all_file_names[j]]['filename']\n",
        "\n",
        "    if image_name in files_in_directory:\n",
        "        img = np.asarray(PIL.Image.open('/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/stage1/'+image_name))\n",
        "        a=a+1\n",
        "        # print(a)\n",
        "    else:\n",
        "        continue\n",
        "\n",
        "    if data[all_file_names[j]]['regions'] != {}:\n",
        "\n",
        "        try:\n",
        "            shape1_x=data[all_file_names[j]]['regions']['0']['shape_attributes']['all_points_x']\n",
        "            shape1_y=data[all_file_names[j]]['regions']['0']['shape_attributes']['all_points_y']\n",
        "        except :\n",
        "            shape1_x=data[all_file_names[j]]['regions'][0]['shape_attributes']['all_points_x']\n",
        "            shape1_y=data[all_file_names[j]]['regions'][0]['shape_attributes']['all_points_y']\n",
        "\n",
        "        ab=np.stack((shape1_x, shape1_y), axis=1)\n",
        "\n",
        "        mask = np.zeros((img.shape[0],img.shape[1]))\n",
        "        img3=cv2.drawContours(mask, [ab.astype(int)], -1, 255, -1)\n",
        "        Mask=mask.astype(np.uint8)\n",
        "        thresh = 150\n",
        "\n",
        "        # get threshold image\n",
        "        ret,thresh_img = cv2.threshold(img, thresh, 255, 0)\n",
        "\n",
        "        contours, hierarchy = cv2.findContours(Mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        imagename=image_name.replace(\"jpg\", \"png\")\n",
        "\n",
        "        cv2.imwrite('/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/stage1/%s' % imagename,mask.astype(np.uint8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6_i_39mN6ZL"
      },
      "source": [
        "# Generate mask image for Stage-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2WUHZOFF85v"
      },
      "outputs": [],
      "source": [
        "# The code generates stage-2 mask images from annotations provided in a JSON file (stage2cancer.json).\n",
        "# It iterates through image files, extracts region coordinates, plots contours on the original image,\n",
        "# and creates corresponding binary masks. These masks are saved in the '/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/stage2/' directory in PNG format\n",
        "\n",
        "\n",
        "\n",
        "# creating stage-2 mask images from annotation\n",
        "with open(\"/content/gdrive/MyDrive/Gastric_Cancer_Project/stage2.json\", \"r\") as read_file:\n",
        "    data = json.load(read_file)\n",
        "\n",
        "all_file_names=list(data.keys())\n",
        "# print(len(all_file_names))\n",
        "\n",
        "Files_in_directory = []\n",
        "for root, dirs, files in os.walk(\"/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/stage2\"):\n",
        "    for filename in files:\n",
        "        Files_in_directory.append(filename)\n",
        "        # print(filename)\n",
        "\n",
        "a=0\n",
        "for j in range(len(all_file_names)):\n",
        "    #print(all_file_names[j])\n",
        "    image_name=data[all_file_names[j]]['filename']\n",
        "\n",
        "    if image_name in Files_in_directory:\n",
        "        img = np.asarray(PIL.Image.open('/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue/stage2/'+image_name))\n",
        "        # a=a+1\n",
        "        # print(a)\n",
        "    else:\n",
        "        print(image_name)\n",
        "        continue\n",
        "\n",
        "    if data[all_file_names[j]]['regions'] != {}:\n",
        "\n",
        "        try:\n",
        "            shape1_x=data[all_file_names[j]]['regions']['0']['shape_attributes']['all_points_x']\n",
        "            shape1_y=data[all_file_names[j]]['regions']['0']['shape_attributes']['all_points_y']\n",
        "        except :\n",
        "            shape1_x=data[all_file_names[j]]['regions'][0]['shape_attributes']['all_points_x']\n",
        "            shape1_y=data[all_file_names[j]]['regions'][0]['shape_attributes']['all_points_y']\n",
        "\n",
        "        fig = plt.figure()\n",
        "        plt.imshow(img.astype(np.uint8))\n",
        "        plt.scatter(shape1_x,shape1_y,zorder=2,color='red',marker = '.', s= 55)\n",
        "\n",
        "        ab=np.stack((shape1_x, shape1_y), axis=1)\n",
        "        img4 = np.zeros((img.shape[0],img.shape[1]))\n",
        "        img2=cv2.drawContours(img4, [ab.astype(int)], -1, (0,255,0), -1)\n",
        "\n",
        "        mask = np.zeros((img.shape[0],img.shape[1]))\n",
        "        img3=cv2.drawContours(mask, [ab.astype(int)], -1, 255, -1)\n",
        "        Mask=mask.astype(np.uint8)\n",
        "        thresh = 150\n",
        "\n",
        "        # get threshold image\n",
        "        ret,thresh_img = cv2.threshold(img, thresh, 255, 0)\n",
        "\n",
        "        # find contours\n",
        "        contours, hierarchy = cv2.findContours(Mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "        imagename=image_name.replace(\"jpg\", \"png\")\n",
        "\n",
        "        cv2.imwrite('/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/stage2/%s' % imagename,mask.astype(np.uint8))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "Y-H4Ib4BpegH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQzqasS8rNma"
      },
      "outputs": [],
      "source": [
        "# These parameters (IMG_WIDTH, IMG_HEIGHT, and IMG_CHANNELS) define the dimensions and color channels of images,\n",
        "# with values set to 128 pixels in width, 128 pixels in height, and 3 color channels (RGB), respectively.\n",
        "IMG_WIDTH = 128\n",
        "IMG_HEIGHT = 128\n",
        "IMG_CHANNELS = 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gkjx1JCb8Atm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTCn9vv3rNmc"
      },
      "outputs": [],
      "source": [
        "# The code initializes file paths for images and masks corresponding to different classes in a gastric cancer project.\n",
        "#It then loads, resizes, and stores the images and masks into numpy arrays (X_train and Y_train) for\n",
        "# subsequent machine learning tasks, ensuring compatibility with a specified image size (IMG_HEIGHT and IMG_WIDTH)\n",
        "classes=['normal','stage1','stage2']\n",
        "pth = '/content/gdrive/MyDrive/Gastric_Cancer_Project/cancer_tissue'\n",
        "imgs=[]\n",
        "for i in classes:\n",
        "\tpath=os.path.join(pth, i)\n",
        "\tfor img in os.listdir(path):\n",
        "\t\tif img=='.ipynb_checkpoints':\n",
        "\t\t\tcontinue\n",
        "\t\timgs.append(os.path.join(path, img))\n",
        "\n",
        "\n",
        "mask_path='/content/gdrive/MyDrive/Gastric_Cancer_Project/masks'\n",
        "masks=[]\n",
        "for j in classes:\n",
        "\tpath1=os.path.join(mask_path, j)\n",
        "\tfor mask in os.listdir(path1):\n",
        "\t\tif mask=='.ipynb_checkpoints':\n",
        "\t\t\tcontinue\n",
        "\t\tmasks.append(os.path.join(path1, mask))\n",
        "\n",
        "\n",
        "# Get and resize train images and masks\n",
        "X_train = np.zeros((len(imgs), IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "Y_train = np.zeros((len(masks), IMG_HEIGHT, IMG_WIDTH, 1), dtype=np.bool_)\n",
        "\n",
        "print(\"X_train\",X_train.shape)\n",
        "print(\"Y_train\",Y_train.shape)\n",
        "\n",
        "\n",
        "print('Getting and resizing train images and masks ... ')\n",
        "sys.stdout.flush()\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "for n, img in tqdm(enumerate(sorted(imgs)), total=len(imgs)):\n",
        "\t\timgs1 = imageio.v2.imread(img)[:,:,:IMG_CHANNELS]\n",
        "\t\t# print(imgs1)\n",
        "\t\timgs1 = resize(imgs1, (IMG_HEIGHT, IMG_WIDTH),  preserve_range=True)\n",
        "\t\tX_train[n] = imgs1\n",
        "\n",
        "i=0\n",
        "for n, img in tqdm(enumerate(sorted(masks)), total=len(masks)):\n",
        "\tmask_ = imread(img)\n",
        "\n",
        "\tmask_ = np.expand_dims(resize(mask_, (IMG_HEIGHT, IMG_WIDTH), mode='constant',preserve_range=True), axis=-1)\n",
        "\t# print(mask_ .shape)\n",
        "\t# print(img)\n",
        "\ti=1+i\n",
        "\tY_train[n] = mask_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxPDlHZs2We6"
      },
      "source": [
        "Multi-taskNet and GlobalNet model for segmenting images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSw5OhTIW1qV"
      },
      "outputs": [],
      "source": [
        "'''This project aims to create a Multi-taskNet model and GlobalNet for image segmentation using a combination of downsampling and upsampling blocks to capture hierarchical features in the input images.\n",
        "The model is trained to output binary segmentation masks for the given input images.\n",
        "Input Processing: The model takes input images with dimensions IMG_HEIGHT x IMG_WIDTH x IMG_CHANNELS. The images are normalized by dividing pixel values by 255.\n",
        "\n",
        "Multi-taskNet Module:\n",
        "The core of the architecture consists of a series of convolutional blocks. These blocks include 1x1, 3x3, and 3x3 convolutions with batch normalization and ReLU activation, forming a feature extraction module.\n",
        "Global Net:\n",
        "Downsampling Path: The model follows a U-Net-like structure with a downsampling path that includes multiple convolutional layers with max-pooling, gradually reducing spatial dimensions while increasing feature channels.\n",
        "Upsampling Path: The upsampling path involves transpose convolutional layers that increase spatial dimensions, followed by concatenation with feature maps from the corresponding downsampling path. This helps to recover spatial information lost during downsampling.\n",
        "Output Layer: The final layer employs a 1x1 convolution with sigmoid activation, producing a binary segmentation mask. The model is compiled using the Adam optimizer and binary cross-entropy loss for segmentation tasks. The summary of the model architecture is printed.'''\n",
        "\n",
        "# Build Multi-taskNet model\n",
        "chanDim = 1\n",
        "stride=(1,1)\n",
        "reg=0.0001\n",
        "bnEps=2e-5\n",
        "bnMom=0.9\n",
        "\n",
        "inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
        "\n",
        "s = Lambda(lambda x: x / 255) (inputs)\n",
        "\n",
        "# the first block of the multitaskNet module are the 3x3 CONVs\n",
        "K=32\n",
        "bn1 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "\t\t\tmomentum=bnMom)(s)\n",
        "act1 = Activation(\"relu\")(bn1)\n",
        "conv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
        "    kernel_regularizer=l2(reg))(act1)\n",
        "\n",
        "# the second block of the multitaskNet module are the 3x3 CONVs\n",
        "K=64\n",
        "bn2 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "    momentum=bnMom)(conv1)\n",
        "act2 = Activation(\"relu\")(bn2)\n",
        "conv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
        "    padding=\"same\", use_bias=False,\n",
        "    kernel_regularizer=l2(reg))(act2)\n",
        "\n",
        "# the third block of the multitaskNet module is another set of 1x1 CONVs\n",
        "K=128\n",
        "bn3 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "    momentum=bnMom)(conv2)\n",
        "act3 = Activation(\"relu\")(bn3)\n",
        "conv3 = Conv2D(K, (1, 1), use_bias=False,\n",
        "    kernel_regularizer=l2(reg))(act3)\n",
        "\n",
        "# the forth block of the multitaskNet module is another set of 3x3 CONVs\n",
        "K=256\n",
        "bn4 = BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
        "    momentum=bnMom)(conv1)\n",
        "act4 = Activation(\"relu\")(bn2)\n",
        "conv4 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
        "    padding=\"same\", use_bias=False,\n",
        "    kernel_regularizer=l2(reg))(act4)\n",
        "\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (conv4)\n",
        "c1 = Dropout(0.1) (c1)\n",
        "c1 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "p1 = MaxPooling2D((2, 2)) (c1)\n",
        "\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "c2 = Dropout(0.1) (c2)\n",
        "c2 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "p2 = MaxPooling2D((2, 2)) (c2)\n",
        "\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "c3 = Dropout(0.2) (c3)\n",
        "c3 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "p3 = MaxPooling2D((2, 2)) (c3)\n",
        "\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "c4 = Dropout(0.2) (c4)\n",
        "c4 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "c5 = Dropout(0.3) (c5)\n",
        "c5 = Conv2D(256, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "\n",
        "u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "\n",
        "c6 = Dropout(0.2) (c6)\n",
        "c6 = Conv2D(128, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "\n",
        "u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "c7 = Dropout(0.2) (c7)\n",
        "c7 = Conv2D(64, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "\n",
        "u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "c8 = Dropout(0.1) (c8)\n",
        "c8 = Conv2D(32, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "\n",
        "u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "c9 = Dropout(0.1) (c9)\n",
        "c9 = Conv2D(16, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "\n",
        "outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
        "\n",
        "\n",
        "# Compile The model\n",
        "model = Model(inputs=[inputs], outputs=[outputs])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(\"The model is defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "US1AWhKirNmh"
      },
      "outputs": [],
      "source": [
        "'''The code sets up a ModelCheckpoint callback during training, saving the best model weights based\n",
        " on accuracy on the validation set to the file 'segementation_weight.h5'.\n",
        "The training process is then initiated for 500 epochs with a batch size of 32,\n",
        "and the checkpoint callback is employed to monitor and save the best-performing model during training.'''\n",
        "\n",
        "checkpoint_filepath='/content/segementation_weight.h5'\n",
        "checkpointer = ModelCheckpoint(checkpoint_filepath, verbose=1, save_best_only=True,monitor='accuracy',mode=\"max\")\n",
        "H = model.fit(X_train, Y_train, batch_size=32,callbacks=[checkpointer],epochs=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loss"
      ],
      "metadata": {
        "id": "G9dZ7qKTgtfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(np.arange(0,500), H.history[\"loss\"], label=\"Loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6oprveyYkpID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Accuracy"
      ],
      "metadata": {
        "id": "3eLmGjLmgv7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use(\"ggplot\")\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(np.arange(0,500), H.history[\"accuracy\"], label=\"Accuracy\")\n",
        "plt.title(\"Training \")\n",
        "plt.xlabel(\"Epoch \")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "F7AwxJS-kiIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh-kGuwECP4D"
      },
      "source": [
        "# Classification Training using FusionNet Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10Wwjy3bEc8z"
      },
      "outputs": [],
      "source": [
        "classes=['normal','stage1','stage2']\n",
        "mask_path='/content/gdrive/MyDrive/Gastric_Cancer_Project/masks/'\n",
        "masks=[]\n",
        "for j in classes:\n",
        "\tpath1=os.path.join(mask_path, j)\n",
        "\tfor mask in os.listdir(path1):\n",
        "\t\tif mask=='.ipynb_checkpoints':\n",
        "\t\t\tcontinue\n",
        "\t\tmasks.append(os.path.join(path1, mask))\n",
        "\n",
        "masks.sort()\n",
        "INPUT_SIZE = 64\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "\n",
        "class SimplePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# resize the image to a fixed size, ignoring the aspect\n",
        "\t\t# ratio\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)\n",
        "\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)\n",
        "\n",
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)\n",
        "\n",
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\n",
        "\n",
        "\t\t\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\t\t\tlabels.append(label)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data), np.array(labels))\n",
        "\n",
        "\n",
        "classNames = [pt.split(os.path.sep)[-2] for pt in masks]\n",
        "classNames = [str(x) for x in np.unique(classNames)]\n",
        "\n",
        "# initialize the image preprocessors\n",
        "aap = AspectAwarePreprocessor(64, 64)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "\n",
        "# load the dataset from disk then scale the raw pixel intensities\n",
        "# to the range [0, 1]\n",
        "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
        "(data, labels) = sdl.load(masks, verbose=500)\n",
        "data = data.astype(\"float\") / 255.0\n",
        "\n",
        "# partition the data into training and testing splits using 75% of\n",
        "# the data for training and the remaining 25% for testing\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=0.25, random_state=42)\n",
        "\n",
        "# convert the labels from integers to vectors\n",
        "trainY = LabelBinarizer().fit_transform(trainY)\n",
        "testY = LabelBinarizer().fit_transform(testY)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdoCLZQ3DWx1"
      },
      "source": [
        "# FusionNet Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras_applications"
      ],
      "metadata": {
        "id": "6JWROu_nQGdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HvnsrCA9c2R"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "import tensorflow as tf\n",
        "from keras import backend\n",
        "from keras_applications import imagenet_utils\n",
        "from keras_applications.imagenet_utils import _obtain_input_shape\n",
        "from tensorflow.python.keras.engine import training\n",
        "from tensorflow.python.keras.layers import VersionAwareLayers\n",
        "from tensorflow.python.keras.utils import layer_utils\n",
        "from tensorflow.python.util.tf_export import keras_export\n",
        "\n",
        "layers = VersionAwareLayers()\n",
        "\n",
        "class FCHeadNet:\n",
        "\t@staticmethod\n",
        "\tdef build(baseModel, classes, D):\n",
        "\t\t# initialize the head model that will be placed on top of\n",
        "\t\t# the base, then add a FC layer\n",
        "\t\theadModel = baseModel.output\n",
        "\t\theadModel = Flatten(name=\"flatten\")(headModel)\n",
        "\t\theadModel = Dense(D, activation=\"relu\")(headModel)\n",
        "\t\theadModel = Dropout(0.5)(headModel)\n",
        "\n",
        "\t\t# add a softmax layer\n",
        "\t\theadModel = Dense(classes, activation=\"softmax\")(headModel)\n",
        "\n",
        "\t\t# return the model\n",
        "\t\treturn headModel\n",
        "\n",
        "def VGG16(\n",
        "    include_top=True,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation='softmax'):\n",
        "\n",
        "  if not (weights in {'imagenet', None} or tf.io.gfile.exists(weights)):\n",
        "    raise ValueError(\n",
        "        'The `weights` argument should be either '\n",
        "        '`None` (random initialization), `imagenet` '\n",
        "        '(pre-training on ImageNet), '\n",
        "        'or the path to the weights file to be loaded.  Received: '\n",
        "        f'weights={weights}')\n",
        "\n",
        "  if weights == 'imagenet' and include_top and classes != 1000:\n",
        "    raise ValueError('If using `weights` as `\"imagenet\"` with `include_top` '\n",
        "                     'as true, `classes` should be 1000.  '\n",
        "                     f'Received `classes={classes}`')\n",
        "  # Determine proper input shape\n",
        "  input_shape = _obtain_input_shape(\n",
        "      input_shape,\n",
        "      default_size=224,\n",
        "      min_size=32,\n",
        "      data_format=backend.image_data_format(),\n",
        "      require_flatten=include_top,\n",
        "      weights=weights)\n",
        "\n",
        "  if input_tensor is None:\n",
        "    img_input = layers.Input(shape=input_shape)\n",
        "  else:\n",
        "    if not backend.is_keras_tensor(input_tensor):\n",
        "      img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
        "    else:\n",
        "      img_input = input_tensor\n",
        "  # Block 1\n",
        "  x = layers.Conv2D(\n",
        "      64, (3, 3), activation='relu', padding='same', name='block1_conv1')(\n",
        "          img_input)\n",
        "  x = layers.Conv2D(\n",
        "      64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
        "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "\n",
        "  # Block 2\n",
        "  x = layers.Conv2D(\n",
        "      128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
        "  x = layers.Conv2D(\n",
        "      128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
        "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "\n",
        "  # Block 3\n",
        "  x = layers.Conv2D(\n",
        "      256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
        "  x = layers.Conv2D(\n",
        "      256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
        "  x = layers.Conv2D(\n",
        "      256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
        "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "\n",
        "  # Block 4\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
        "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "\n",
        "  # Block 5\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
        "  x = layers.Conv2D(\n",
        "      512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
        "  x = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "\n",
        "  if include_top:\n",
        "    # Classification block\n",
        "    x = layers.Flatten(name='flatten')(x)\n",
        "    x = layers.Dense(4096, activation='relu', name='fc1')(x)\n",
        "    x = layers.Dense(4096, activation='relu', name='fc2')(x)\n",
        "\n",
        "    imagenet_utils.validate_activation(classifier_activation, weights)\n",
        "    x = layers.Dense(classes, activation=classifier_activation,\n",
        "                     name='predictions')(x)\n",
        "  else:\n",
        "    if pooling == 'avg':\n",
        "      x = layers.GlobalAveragePooling2D()(x)\n",
        "    elif pooling == 'max':\n",
        "      x = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "  # Ensure that the model takes into account\n",
        "  # any potential predecessors of `input_tensor`.\n",
        "  if input_tensor is not None:\n",
        "    inputs = layer_utils.get_source_inputs(input_tensor)\n",
        "  else:\n",
        "    inputs = img_input\n",
        "  # Create model.\n",
        "  model = training.Model(inputs, x, name='vgg16')\n",
        "\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTN0lYUl_40e"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "with warnings.catch_warnings():\n",
        "\twarnings.simplefilter(\"ignore\")\n",
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(64, 64, 3)))\n",
        "\n",
        "# initialize the new head of the network, a set of FC layers\n",
        "# followed by a softmax classifier\n",
        "headModel = FCHeadNet.build(baseModel, len(classNames), 256)\n",
        "\n",
        "# place the head FC model on top of the base model -- this will\n",
        "# become the actual model we will train\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# loop over all layers in the base model and freeze them so they\n",
        "# will *not* be updated during the training process\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False\n",
        "\n",
        "# compile our model (this needs to be done after our setting our\n",
        "# layers to being non-trainable\n",
        "print(\"[INFO] compiling model...\")\n",
        "opt = Adam(learning_rate=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])\n",
        "\n",
        "print(\"[INFO] training network...\")\n",
        "H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
        "  batch_size=32, epochs=100, verbose=1)\n",
        "\n",
        "print(\"[INFO] evaluating after fine-tuning...\")\n",
        "predictions = model.predict(testX, batch_size=32)\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=classNames))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training history\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(H.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(H.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(H.history['loss'], label='Training Loss')\n",
        "plt.plot(H.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eLTUuU1em58p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pair_confusion_matrix\n",
        "import seaborn as sns\n",
        "classnames = [\"normal\",\"stage1\",\"stage 2\"]\n",
        "print('Confusion Matrix:')\n",
        "CM = confusion_matrix(testY.argmax(axis=1),predictions.argmax(axis=1))\n",
        "# drawing confusion matrix\n",
        "sns.heatmap(CM, square = True , annot=True, fmt=\"d\" ,cmap=\"Blues\", xticklabels=classnames, yticklabels=classnames)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9IvSsMw9LkA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics calculation\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
        "accuracy = accuracy_score(predictions.argmax(axis = 1),testY.argmax(axis = 1))\n",
        "precision = precision_score(predictions.argmax(axis = 1),testY.argmax(axis = 1),average = \"weighted\")\n",
        "recall = recall_score(predictions.argmax(axis = 1),testY.argmax(axis = 1),average = \"weighted\")\n",
        "f1_score = f1_score(predictions.argmax(axis = 1),testY.argmax(axis = 1),average = \"weighted\")"
      ],
      "metadata": {
        "id": "V-Cy75j8Zsyu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy = [round(accuracy,2)*100,round(precision,2)*100,round(recall,2)*100,round(f1_score,2)*100]\n",
        "objects =  ('Accuracy', 'Precision','Recall','F1-score')\n",
        "y_pos = np.arange(len(objects))\n",
        "plt.figure(figsize = (15,5))\n",
        "plt.bar(y_pos, Accuracy, align='center', alpha=0.5,color = ['black', 'red', 'green', 'blue'])\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Metrics Name')\n",
        "plt.title('Performance metrics comparison')\n",
        "\n",
        "# Insert data labels above each bar\n",
        "for i, value in enumerate(Accuracy):\n",
        "    plt.text(i, value+0.01, str(value), ha='center')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "IodpL_XfZrgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "_-CmPmnupj8k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJqgz4MzCWLA"
      },
      "source": [
        "Prediciton"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/gdrive/MyDrive/Gastric_Cancer_Project/gastric_test_data.zip -d ."
      ],
      "metadata": {
        "id": "dHDsxOGrmA_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "8FDZTXvLocO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AspectAwarePreprocessor:\n",
        "\tdef __init__(self, width, height, inter=cv2.INTER_AREA):\n",
        "\t\t# store the target image width, height, and interpolation\n",
        "\t\t# method used when resizing\n",
        "\t\tself.width = width\n",
        "\t\tself.height = height\n",
        "\t\tself.inter = inter\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# grab the dimensions of the image and then initialize\n",
        "\t\t# the deltas to use when cropping\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\tdW = 0\n",
        "\t\tdH = 0\n",
        "\n",
        "\t\t# if the width is smaller than the height, then resize\n",
        "\t\t# along the width (i.e., the smaller dimension) and then\n",
        "\t\t# update the deltas to crop the height to the desired\n",
        "\t\t# dimension\n",
        "\t\tif w < h:\n",
        "\t\t\timage = imutils.resize(image, width=self.width,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdH = int((image.shape[0] - self.height) / 2.0)\n",
        "\n",
        "\t\t# otherwise, the height is smaller than the width so\n",
        "\t\t# resize along the height and then update the deltas\n",
        "\t\t# crop along the width\n",
        "\t\telse:\n",
        "\t\t\timage = imutils.resize(image, height=self.height,\n",
        "\t\t\t\tinter=self.inter)\n",
        "\t\t\tdW = int((image.shape[1] - self.width) / 2.0)\n",
        "\n",
        "\t\t# now that our images have been resized, we need to\n",
        "\t\t# re-grab the width and height, followed by performing\n",
        "\t\t# the crop\n",
        "\t\t(h, w) = image.shape[:2]\n",
        "\t\timage = image[dH:h - dH, dW:w - dW]\n",
        "\n",
        "\t\t# finally, resize the image to the provided spatial\n",
        "\t\t# dimensions to ensure our output image is always a fixed\n",
        "\t\t# size\n",
        "\t\treturn cv2.resize(image, (self.width, self.height),\n",
        "\t\t\tinterpolation=self.inter)"
      ],
      "metadata": {
        "id": "j8_UWUhSpLSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "z_QfxCdNodxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleDatasetLoader:\n",
        "\tdef __init__(self, preprocessors=None):\n",
        "\t\t# store the image preprocessor\n",
        "\t\tself.preprocessors = preprocessors\n",
        "\n",
        "\t\t# if the preprocessors are None, initialize them as an\n",
        "\t\t# empty list\n",
        "\t\tif self.preprocessors is None:\n",
        "\t\t\tself.preprocessors = []\n",
        "\n",
        "\tdef load(self, imagePaths, verbose=-1):\n",
        "\t\t# initialize the list of features and labels\n",
        "\t\tdata = []\n",
        "\t\tlabels = []\n",
        "\n",
        "\t\t# loop over the input images\n",
        "\t\tfor (i, imagePath) in enumerate(imagePaths):\n",
        "\t\t\t# load the image and extract the class label assuming\n",
        "\t\t\t# that our path has the following format:\n",
        "\t\t\t# /path/to/dataset/{class}/{image}.jpg\n",
        "\t\t\timage = cv2.imread(imagePath)\n",
        "\n",
        "\n",
        "\t\t\t# check to see if our preprocessors are not None\n",
        "\t\t\tif self.preprocessors is not None:\n",
        "\t\t\t\t# loop over the preprocessors and apply each to\n",
        "\t\t\t\t# the image\n",
        "\t\t\t\tfor p in self.preprocessors:\n",
        "\t\t\t\t\timage = p.preprocess(image)\n",
        "\n",
        "\t\t\t# treat our processed image as a \"feature vector\"\n",
        "\t\t\t# by updating the data list followed by the labels\n",
        "\t\t\tdata.append(image)\n",
        "\n",
        "\t\t\t# show an update every `verbose` images\n",
        "\t\t\tif verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n",
        "\t\t\t\tprint(\"[INFO] processed {}/{}\".format(i + 1,\n",
        "\t\t\t\t\tlen(imagePaths)))\n",
        "\n",
        "\t\t# return a tuple of the data and labels\n",
        "\t\treturn (np.array(data))"
      ],
      "metadata": {
        "id": "aJWH10XYpKcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "lkIw6tW5ofFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageToArrayPreprocessor:\n",
        "\tdef __init__(self, dataFormat=None):\n",
        "\t\t# store the image data format\n",
        "\t\tself.dataFormat = dataFormat\n",
        "\n",
        "\tdef preprocess(self, image):\n",
        "\t\t# apply the Keras utility function that correctly rearranges\n",
        "\t\t# the dimensions of the image\n",
        "\t\treturn img_to_array(image, data_format=self.dataFormat)"
      ],
      "metadata": {
        "id": "biHPZsAIpCQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "foQmFVe8oqWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seg_model = load_model('/content/gdrive/MyDrive/Gastric_Cancer_Project/segementation_weight.h5')\n",
        "class_model = load_model('/content/gdrive/MyDrive/Gastric_Cancer_Project/class.h5')"
      ],
      "metadata": {
        "id": "6mvlhC9HXIct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "run"
      ],
      "metadata": {
        "id": "aXMVRD7forlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from imutils import paths\n",
        "#give  input image path\n",
        "imgs11 = \"/content/gastric_test_data/image_0_6994.jpg\"\n",
        "X_test1 = np.zeros((1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.uint8)\n",
        "sizes_test1 = []\n",
        "img1 = imread(imgs11)[:,:,:IMG_CHANNELS]\n",
        "\n",
        "sizes_test1.append([img1.shape[0], img1.shape[1]])\n",
        "img1 = resize(img1, (IMG_HEIGHT, IMG_WIDTH), mode='constant', preserve_range=True)\n",
        "X_test1[0] = img1\n",
        "\n",
        "# #Predict on train, val and test\n",
        "# model = load_model('/content/gdrive/MyDrive/gastric cancer/seg_weight.h5')\n",
        "\n",
        "preds_test = seg_model.predict(X_test1, verbose=1)\n",
        "\n",
        "preds_test_upsampled1=[]\n",
        "preds_test_t = (preds_test > 0.5).astype(np.uint8)\n",
        "preds_test_upsampled1.append(resize(np.squeeze(preds_test_t[0]),  (sizes_test1[0][0], sizes_test1[0][1]), mode='constant',\n",
        "    preserve_range=True))\n",
        "\n",
        "# Display original image\n",
        "fig=plt.figure(figsize=(8, 8))\n",
        "fig.add_subplot(1, 2, 1)\n",
        "plt.imshow(X_test1[0])\n",
        "plt.title('Original Image')\n",
        "\n",
        "# Display predicted mask\n",
        "fig.add_subplot(1, 2, 2)\n",
        "plt.imshow(np.squeeze(preds_test[0]), cmap='gray')\n",
        "plt.title('Predicted Mask')\n",
        "plt.show()\n",
        "# Save mask image\n",
        "mask_image_path = \"/content/testmask.jpg\"\n",
        "image = Image.fromarray((preds_test_upsampled1[0] * 255).astype(np.uint8), 'L')\n",
        "image.save(mask_image_path)\n",
        "onesegmask=['testmask.jpg']\n",
        "aap = AspectAwarePreprocessor(64,64)\n",
        "iap = ImageToArrayPreprocessor()\n",
        "sdl = SimpleDatasetLoader(preprocessors=[aap, iap])\n",
        "data = sdl.load(onesegmask, verbose=500)\n",
        "data = data.astype(\"float\") / 255.0\n",
        "preds = class_model.predict(data).argmax(axis=1)\n",
        "classNames = [\"Normal\",\"Stage1\",\"Stage2\"]\n",
        "print(\"Predicted Stage:\",classNames[preds[0]])"
      ],
      "metadata": {
        "id": "EGKHcqGqn25u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NDVCRVpgoU9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}